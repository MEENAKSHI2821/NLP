{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Some Packages\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer , SnowballStemmer\n",
    "stemmer_s = SnowballStemmer('english')\n",
    "from string import punctuation\n",
    "stop_nltk=stopwords.words(\"english\")\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tw=TweetTokenizer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('/kaggle/input/budget-txt/budget_speech.txt',error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" \".join(data['Budget 2020-2021'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(txt)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe words spoken most are coming in bigger font and we have some words like department and others which have to be removed as we know the data is about budget and there words are gonna br common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little Beautification.\n",
    "word_cloud = WordCloud(width=800,height=800,background_color='white',\n",
    "                      max_words=150).generate(txt)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(word_cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting words and its frequency in dict-key-values.\n",
    "y = {} \n",
    "for i in txt.split(' '): \n",
    "    y[i] = y.get(i,0)+1\n",
    "#conversion:\n",
    "freq = {'words':list(y.keys()) , 'freq':list(y.values())}\n",
    "\n",
    "mydata=pd.DataFrame(freq)\n",
    "mydata.sort_values(by='freq',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see some \"STOPWORDS\" and space coming most times which has to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1/ Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying tokenizer\n",
    "text=word_tokenize(txt.lower()) # Converting all words to lower case for Uniform Casing.\n",
    "print(len(txt),len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenizating we are left with 6119 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(text)\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still we have punctuations and ofcourse STOPWORDS which we will remove now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2/Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "stop_nltk=stopwords.words(\"english\")\n",
    "upd_stop = stop_nltk + ['department','government','proposed','under','centeral','will','ministry','provide','rate'] \n",
    "txt_upd = [term for term in text if term not in upd_stop and \\\n",
    "               term not in list(punctuation) and len(term)>2]\n",
    "len(set(txt_upd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtxt=[lemm.lemmatize(word) for word in txt_upd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New WordCloud\n",
    "mytxt=\" \".join(newtxt)\n",
    "#Initiating WordCloud\n",
    "word_cloud = WordCloud().generate(mytxt)\n",
    "#Beautifying\n",
    "word_cloud = WordCloud(width=800,height=800,background_color='white',\n",
    "                      max_words=150).generate(mytxt)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(word_cloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From this we can infer that \n",
    "    - good word is used - which is a positive sentiment\n",
    "    - tax is been talked about the most\n",
    "    - different schemes could have be launched or reviewed for different development purposes \n",
    "    - alse technology can be seen as one of mostly used word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot\n",
    "fdist = FreqDist(newtxt)\n",
    "freq = {'words':list(fdist.keys()) , 'freq':list(fdist.values())}\n",
    "df=pd.DataFrame(freq)\n",
    "dat=df.sort_values(by='freq',ascending=False).head(25)\n",
    "plt.figure(figsize=(10,12))\n",
    "sns.barplot(data=dat,x='freq',y='words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Bigrams for better inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(newtxt)\n",
    "#Getting the BOW:-\n",
    "count_vect.get_feature_names()\n",
    "#Getting the DataFrame\n",
    "DTM = pd.DataFrame(X.toarray(),columns=count_vect.get_feature_names()) #datatermmatrix\n",
    "# TDM=term document matrix\n",
    "TDM = DTM.T\n",
    "TDM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams\n",
    "count_vect_bg = CountVectorizer(ngram_range=(2,2),max_features=25)\n",
    "X_bg = count_vect_bg.fit_transform(newtxt)\n",
    "DTM_bg = pd.DataFrame(X_bg.toarray(),columns=count_vect_bg.get_feature_names())\n",
    "DTM_bg.drop(columns=['01 04','04 2020','19 20','2019 20','2020 21'],inplace=True)#Dont need date columns so dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "x=DTM_bg.sum().sort_values(ascending=False).head(25)\n",
    "y=x.reset_index()\n",
    "y.head(1)\n",
    "plt.pie(y[0],autopct='%1.1f%%', startangle=90, pctdistance=0.85,shadow=True)\n",
    "plt.legend(y['index'],loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From Pie chart we can observe:\n",
    "    - Different sub sections being talked about\n",
    "    - Stress being given on 'Swach Bharat Abhiyaan' as anti dumping comes into picture.\n",
    "    - pm kisaan yojna and start-ups given importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Sentiment-score of the speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "def get_vader_sentiment(sent):\n",
    "    return analyser.polarity_scores(sent)['compound']\n",
    "print(get_vader_sentiment(mytxt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a positive .99 score which denotes a overall positive vibe throughout the speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and analysis major topics of the budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect =TfidfVectorizer()\n",
    "X=tfidf_vect.fit_transform(newtxt)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4,random_state=0)\n",
    "y_means = kmeans.fit_predict(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW=tfidf_vect.get_feature_names()\n",
    "num_clusters = 4\n",
    "arr=kmeans.cluster_centers_\n",
    "ordered_clu = arr.argsort()[:,::-1]\n",
    "for i in range(num_clusters):\n",
    "    print('Topics :',i)\n",
    "    for i in ordered_clu[i,:5]:\n",
    "        print(BOW[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
